{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QvSE04uu_Pe0"
      },
      "source": [
        "# Week 3 Seminar Notebook\n",
        "This week we are going to be thinking about and working with n-gram language models."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9x3yCQbgF-aE"
      },
      "source": [
        "# Lists\n",
        "\n",
        "You have heard in your research methods Python session about the different data types in Python. Another important  programming concept is the data structure - the structures used to store and organise data in our programs. The first of these we have encountered is the list. I have used this term informally in previous sessions because the everyday English word list capture quite well what a Python list is: an ordered store of entities, where those entities can be e.g. numbers, letter, words, other lists. We represent it using square brackets, such that we would create of list of integers from 1 to 10 like this:\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "akK3qq-yHcNh"
      },
      "outputs": [],
      "source": [
        "nums=[1,2,3,4,5,6,7,8,9,10]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DH6YV6PdHzZe"
      },
      "source": [
        "We can represent the first five letters of the alphabet like this:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Yqx00ezYH6I9"
      },
      "outputs": [],
      "source": [
        "alphabet=[\"a\",\"b\",\"c\",\"d\",\"e\"]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wAa6oiXBIDoC"
      },
      "source": [
        "We can represent a sentence like this:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xO2OOTAUHgX_"
      },
      "outputs": [],
      "source": [
        "sentence = [\"this\", \"is\", \"a\", \"sentence\"]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9umW5JK1IKXp"
      },
      "source": [
        "And we can represent a series of sentences like this"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B8oq0HR6IKmp"
      },
      "outputs": [],
      "source": [
        "sentences = [[\"this\", \"is\", \"a\", \"sentence\"],[\"this\",\"is\",\"another\",\"sentence\"]]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ku11ujdgJRJN"
      },
      "source": [
        "We can print the contents of a list as a single string. The character in the quotes before \".join\" sets the character to be printed between the elements of the list. Here we use a space."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5EWSCRztJWv6"
      },
      "outputs": [],
      "source": [
        "str.join(\" \", sentence)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6kGuJorOHgX_"
      },
      "source": [
        "We can also select elements from within the list. The entries in a list are indexed numberically starting with zero. So the first element is sentence[0] and the last element of this four element list is sentence[3]. These can then be select for printing as follows:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "69l83SK-HgYA"
      },
      "outputs": [],
      "source": [
        "sentence[2]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GS2RC2T8HgYA"
      },
      "source": [
        "We can also select subsequences of entries, by specifying a range as follows. Notice that the second character in the range isn't included - so 0:2 means from 0 up to the number before 2."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Zo9ZgHryHgYA"
      },
      "outputs": [],
      "source": [
        "str.join(\" \", sentence[0:2])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ciEFnb99HgYA"
      },
      "source": [
        "This allows us to, for example, insert elements in the middle of sentences as follows:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hn66kOmDHgYA"
      },
      "outputs": [],
      "source": [
        "print(str.join(\" \", sentence[0:3]) + \" short \" + sentence[3])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JM8O3DHrHfeM"
      },
      "source": [
        "An important thing to note is that a string in Python is a list of characters. So that you can select character from within strings as follows:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y0Rn22l-Lira"
      },
      "outputs": [],
      "source": [
        "sentence2 = \"the dog lay on the rug\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pDgdKw__LuNc"
      },
      "outputs": [],
      "source": [
        "sentence2[5]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LjB0bTCV2h33"
      },
      "source": [
        "You can find the length of a list as follows"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f1YpuKdj2gaB"
      },
      "outputs": [],
      "source": [
        "len(sentence2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RssxlPiU2C8V"
      },
      "source": [
        "You have also briefly encountered for loops. You can iterate over the elements in a list as follows:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iGpLMLbc2HE_"
      },
      "outputs": [],
      "source": [
        "for elem in sentence2:\n",
        "    print(elem)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "erjcXH4t2dxN"
      },
      "source": [
        "Or by index as follows"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1Ahy6EtB2U2u"
      },
      "outputs": [],
      "source": [
        "for i in range(len(sentence2)):\n",
        "    print(sentence2[i])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rRhjvS71Mwk8"
      },
      "source": [
        "### Dictionaries\n",
        "A second useful data structure is the dictionary. This stores data in key and value pairs. There is a flexibility in the data types that can be keys and can be values, for example the former could be a string or an int. The latter could be a list or even another dictionary."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GzZyc9SENAqp"
      },
      "outputs": [],
      "source": [
        "thisdict = {\n",
        "  \"brand\": \"Ford\",\n",
        "  \"model\": \"Mustang\",\n",
        "  \"year\": 1964\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hkBZ6gCANMrL"
      },
      "outputs": [],
      "source": [
        "print(thisdict)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TpnWMsHuKxL5"
      },
      "source": [
        "You can obtain the keys as a standalone list"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NtrlqssuK3Il"
      },
      "outputs": [],
      "source": [
        "thisdict.keys()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BoyoyS9qK68-"
      },
      "source": [
        "And the same for the values"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tGy3_9SBK92h"
      },
      "outputs": [],
      "source": [
        "thisdict.values()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4LY_zLsR10_7"
      },
      "source": [
        "You can iterate over the keys and values of dictionaries as follows:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xcamhP911V--"
      },
      "outputs": [],
      "source": [
        "for key, value in thisdict.items():\n",
        "    print(key + \" \" + str(value))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y3zo9OcfBIsO"
      },
      "source": [
        "One useful additional thing to consider is that there are different kinds of dictionaries in the Collections library. We will make use of one special kind of dictionary - the default dictionary which returns a default value when asked for a missing key."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "InlBGl8n9nGy"
      },
      "source": [
        "### The Shannon game\n",
        "\n",
        "Shannon (1951) described an ingenious way of estimating the entropy of English by using human predictions.\n",
        "\n",
        "Shannon, C. E. (1951). Prediction and entropy of printed English. Bell system technical journal, 30(1), 50-64.\n",
        "\n",
        "To play the Shannon game in your own time:\n",
        "\n",
        "Copy this URL into a browser window and press return\n",
        "\n",
        "https://github.com/lianghuang3/shannon_game/archive/refs/heads/main.zip\n",
        "\n",
        "Unpack repository to somewhere on local machine\n",
        "\n",
        "If you are on University Computer or have VS Code installed\n",
        "Open VSCode (e.g. via Anaconda Navigator)\n",
        "Select File -> Open Folder and then select the shannon_game folder\n",
        "\n",
        "Open shannon_game.py and press Run\n",
        "\n",
        "Follow instructions given by the program once running."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rGwfrYchCMDb"
      },
      "source": [
        "### Calculate Entropy of English from a sample text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H4vb1HXphOEf"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "# download from from the internt\n",
        "!wget https://www.gutenberg.org/files/2554/2554-0.txt\n",
        "# read in the file\n",
        "f = open('2554-0.txt')\n",
        "c_and_p = f.read()\n",
        "# select the first chapter - possible because I determined range\n",
        "chapter_one = c_and_p[5464:23725]\n",
        "# convert text to lower case\n",
        "chapter_one=chapter_one.lower()\n",
        "# remove all characters except from a-z and space\n",
        "chapter_one=re.sub('[^a-z ]','', chapter_one)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0pjZJj5Ynpcn"
      },
      "outputs": [],
      "source": [
        "chapter_one"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "irvJ9z_xIsXj"
      },
      "source": [
        "We are going to be assigning probabilities to sentences using bigram models, so we need to extract unigram and bigram counts. We will store them in dictionaries."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f4LOgpkKokBv"
      },
      "outputs": [],
      "source": [
        "# First we will build a dictionary of unigram counts\n",
        "# We want to use a special kind of Dictionary called a default dictionary, so we have to import this\n",
        "from collections import defaultdict\n",
        "# Determine the total number of tokens (characters in this case) in the text\n",
        "total_unigrams = len(chapter_one)\n",
        "# Create an empty default dictionary\n",
        "unigrams = defaultdict(int)\n",
        "# Iterate through values of i from zero to the length of the text\n",
        "for i in range(total_unigrams):\n",
        "    # For each element in the list (characters in the chapter), increase the count of that word in our dictionary by one\n",
        "    unigrams[chapter_one[i]] += 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "afbjVTZRL4VY"
      },
      "outputs": [],
      "source": [
        "unigrams"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AskxxL7Ol_uK"
      },
      "outputs": [],
      "source": [
        "# Next we will build a dictionary of bigram counts\n",
        "# We want to use a special kind of Dictionary called a default dictionary, so we have to import this\n",
        "from collections import defaultdict\n",
        "# Determine the total number of bigram tokens (character bigrams in this case) in the text. This is the number of words minus 1\n",
        "total_bigrams = len(chapter_one) - 1\n",
        "# Create an empty default dictionary\n",
        "bigrams = defaultdict(int)\n",
        "# Iterate through values of i from zero to the length of the text minus 1\n",
        "for i in range(total_bigrams):\n",
        "     # For each element in the list (characters in the chapter) extract a bigram consisting of that element and the next and increase the count of that bigram in our dictionary by one\n",
        "    bigrams[chapter_one[i:i+2]] += 1\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M4eU0xzDMnH9"
      },
      "outputs": [],
      "source": [
        "bigrams"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I_fuwg3WMtrb"
      },
      "source": [
        "Now we want to use these counts to calculate the entropy of English.\n",
        "\n",
        "If we were to calculate the entropy assuming every character was independent and occurred an equal number of times, the calculation would be very simple:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FFi77r3AswJY"
      },
      "outputs": [],
      "source": [
        "import math\n",
        "-math.log(1/27,2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dlbIANQjNC7Y"
      },
      "source": [
        "We know that letters do not occur the same number of times (are not equiprobable) so the next thing we might try is to calculate the entropy reflecting the unequal rates but still assuming independence."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EI8_4dp1slMd"
      },
      "outputs": [],
      "source": [
        "# import the math library so that we can use the math.log function\n",
        "import math\n",
        "# initialise entropy to be zero\n",
        "H=0.0\n",
        "# Iterate over all key value pairs in our unigram count dictionary\n",
        "for key, value in unigrams.items():\n",
        "  # Calculate the unigram surprisal of each letter and add it to the entropy weighting by it relative frequency\n",
        "  H += -(value/total_unigrams * math.log(value/total_unigrams, 2))\n",
        "H"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PwsasxmoNWgi"
      },
      "source": [
        "We know that characters are not independent - for each h is more likely to occur following t than following x. This is why we use bigram probabilities."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kWM5gYb8oWr5"
      },
      "outputs": [],
      "source": [
        "# import the math library so that we can use the math.log function\n",
        "import math\n",
        "# initialise entropy to be zero\n",
        "H=0.0\n",
        "# Iterate over all key value pairs in our unigram count dictionary\n",
        "for key, value in bigrams.items():\n",
        "  # Identity the first element in the bigram\n",
        "  unikey = key[:1]\n",
        "  # Calculate the bigram surprisal for each bigram and add it to the entropy weighting by it relative frequency\n",
        "  H += -(value/total_bigrams * math.log(value/unigrams[unikey], 2))\n",
        "H"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MT1xfkYGNjUZ"
      },
      "source": [
        "The estimate is still higher than that we saw in the Shannon game. Why?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_iL4-2j9NrId"
      },
      "source": [
        "Problem 1: Estimate entropy from a trigram character model. First of all you will need to rewrite the code to extract trigram counts."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C65BEbi5N4fU"
      },
      "outputs": [],
      "source": [
        "# We want to use a special kind of Dictionary called a default dictionary, so we have to import this\n",
        "from collections import defaultdict\n",
        "# Determine the total number of trigram tokens (character trigrams in this case) in the text.\n",
        "total_trigrams = ???\n",
        "# Create an empty default dictionary\n",
        "trigrams = defaultdict(int)\n",
        "# Iterate through values of i from zero to ??\n",
        "for i in range(total_trigrams):\n",
        "     # For each element in the list (characters in the chapter) extract a trigram consisting of that element and the next 2 and increase the count of that bigram in our dictionary by one\n",
        "    trigrams[chapter_one[????]] += 1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RcmRZAvxOgnj"
      },
      "source": [
        "Next you will need to use these to calculate the trigram probabilities and combine to calculate the entropy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M9uTPRxoOsjA"
      },
      "outputs": [],
      "source": [
        "import math\n",
        "H=0.0\n",
        "for key, value in trigrams.items():\n",
        "  bigramkey = ????\n",
        "  H += -(value/??? * math.log(value/???, 2))\n",
        "H"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gyizHKmuUuoe"
      },
      "source": [
        "As well as using the counts to estimate the global Entropy of English we can also calculate the log probability of individual sentences including those that weren't in the text from which we estimated our model:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m3Bip0mItzyO"
      },
      "outputs": [],
      "source": [
        "# Define our sentence (a list of characters)\n",
        "sentence3=\" the cat sat on the mat\"\n",
        "# Initialise our log probability\n",
        "log_prob=0.0\n",
        "# Iterate through the sentence from 0 to the length of the sentence minus 1 (because working with bigrams)\n",
        "for i in range(len(sentence3)-1):\n",
        "  #Extract the current bigram of the input sentence\n",
        "  key = sentence3[i:i+2]\n",
        "  # Identity the first element in the bigram\n",
        "  unigram = key[:1]\n",
        "  # Calculate the log bigram probability for the bigram and add it to the log bigram probability for the sentence\n",
        "  log_prob += -math.log(bigrams[key]/unigrams[unigram],2)\n",
        "log_prob\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fzEnusR0d-X7"
      },
      "source": [
        "Because we are adding surprisal for each new element in the sentence, the result is length dependent. For general purpose language model evaluation we need to adjust for length, giving us a measure called perplexity. More information on this here:\n",
        "\n",
        "https://githubtocolab.com/cbannard/lela60331_25-26/blob/main/Perplexity.ipynb\n",
        "\n",
        "For now, just note that it is calculated by raising 2 to the log probability of the sentence divided by its length:"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pow(2,log_prob/len(sentence3))"
      ],
      "metadata": {
        "id": "han-nKWhX3KS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N5r2qMzYxoG2"
      },
      "source": [
        "### Language modelling for words\n",
        "\n",
        "We are now going to switch to building and exploiting word-based language models. In order to do this we are going to have generate a list of words instead of a list of characters. We are going to do this using re.split()\n",
        "\n",
        "### re.split()\n",
        "re.split() takes a regular expression as a first argument (if you don't have a precompiled pattern) and string as second (or first if you have a precompiled pattern) argument, and splits the string into tokens divided by all substrings matched by the regular expression."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ToMjvvtfxrIA"
      },
      "outputs": [],
      "source": [
        "# download from from the internt\n",
        "!wget https://www.gutenberg.org/files/2554/2554-0.txt\n",
        "# read in the file\n",
        "f = open('2554-0.txt')\n",
        "c_and_p = f.read()\n",
        "# select the first chapter - possible because I determined range\n",
        "chapter_one = c_and_p[5464:23725]\n",
        "# convert text to lower case\n",
        "chapter_one=chapter_one.lower()\n",
        "chapter_one=re.sub('\\\\n',' ', chapter_one)\n",
        "chapter_one=re.sub(\"\\\\. \",\" eol \", chapter_one)\n",
        "chapter_one=re.sub('[^a-z ]','', chapter_one)\n",
        "chapter_one=re.sub(' +', ' ',chapter_one)\n",
        "chapter_one=re.split(\" \", chapter_one)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tkHx5PhgWbaD"
      },
      "source": [
        "With the text in this new tokenised format, the same algorithm can be applied to extract unigram and bigram counts."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zy5066AoyvRT"
      },
      "outputs": [],
      "source": [
        "from collections import defaultdict\n",
        "total_unigrams = len(chapter_one) - 1\n",
        "unigrams = defaultdict(int)\n",
        "for i in range(total_unigrams):\n",
        "    unigrams[chapter_one[i]] += 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IQOwFmDNy0QO"
      },
      "outputs": [],
      "source": [
        "from collections import defaultdict\n",
        "total_bigrams = len(chapter_one) - 2\n",
        "bigrams = defaultdict(int)\n",
        "for i in range(total_bigrams):\n",
        "    bigrams[str.join(\" \",chapter_one[i:i+2])] += 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gZiLuezcWzIR"
      },
      "outputs": [],
      "source": [
        "unigrams"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kzFoLndPWz8m"
      },
      "outputs": [],
      "source": [
        "bigrams"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M-eQ9svvW1Oy"
      },
      "source": [
        "And the counts can be used to assign probabilities to sentences in exactly the same way:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rcLB9nfCzqPU"
      },
      "outputs": [],
      "source": [
        "sentence3=[\"a\",\"man\",\"was\",\"in\",\"the\",\"house\"]\n",
        "log_prob=0.0\n",
        "for i in range(len(sentence3)-1):\n",
        "  key = str.join(\" \",sentence3[i:i+2])\n",
        "  unigram = sentence3[i]\n",
        "  log_prob += -math.log(bigrams[key]/unigrams[unigram],2)\n",
        "log_prob"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "prLMG1W-Qh2t"
      },
      "source": [
        "Problem 2: Because we are using bigrams the first element for which we calculate the probability for is the second word. However to give the probability for the full sentence we want to also consider the first word. Update the code so that it does this. You may want to make reference to the lecture slides for the week in order to remind yourself of how we do this."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t0MJt9ia6Oh_"
      },
      "source": [
        "Now try calculating the log probability for a different sentence of your own creation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ia4vrQw86YM7"
      },
      "outputs": [],
      "source": [
        "sentence3=[]\n",
        "log_prob=0.0\n",
        "for i in range(len(sentence3)-1):\n",
        "  key = str.join(\" \",sentence3[i:i+2])\n",
        "  unigram = sentence3[i]\n",
        "  log_prob += -math.log(bigrams[key]/unigrams[unigram],2)\n",
        "log_prob"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ycwY-IA36Y2a"
      },
      "source": [
        "This probably won't work because of unseen bigrams.\n",
        "\n",
        "We need to solve this problem via smoothing.\n",
        "\n",
        "For example, add-one smoothing:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RMhPLL_U5PF5"
      },
      "outputs": [],
      "source": [
        "sentence3=[\"a\",\"man\",\"was\",\"in\",\"the\",\"house\",\"when\",\"the\",\"day\",\"arrived\"]\n",
        "log_prob=0.0\n",
        "for i in range(len(sentence3)-1):\n",
        "  key = str.join(\" \",sentence3[i:i+2])\n",
        "  unigram = sentence3[i]\n",
        "  # We add a count of one to each bigram and then add the vocabulary size (number of unique words) to the denominator\n",
        "  log_prob += -math.log((bigrams[key]+1)/(unigrams[unigram]+len(unigrams.keys())),2)\n",
        "log_prob"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dCCDnDh68ZtS"
      },
      "source": [
        "Or alternatively, back off smoothing with interpolation:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F9a7sTCL_8yu"
      },
      "outputs": [],
      "source": [
        "sentence3=[\"a\",\"man\",\"was\",\"in\",\"the\",\"house\",\"when\",\"the\",\"day\",\"arrived\"]\n",
        "log_prob=0.0\n",
        "# These lambdas can change but should sum to 1\n",
        "lambda1 = 0.5\n",
        "lambda2 = 1 - lambda1\n",
        "\n",
        "for i in range(len(sentence3)-1):\n",
        "  key = str.join(\" \",sentence3[i:i+2])\n",
        "  unigram = sentence3[i]\n",
        "  # We combine the unigram and the bigram probabilities, weighting each equally.\n",
        "  log_prob += -math.log((bigrams[key]/unigrams[unigram])*lambda1 + (unigrams[unigram]/total_unigrams)*lambda2,2)\n",
        "log_prob"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "77IdH9nXXFbV"
      },
      "source": [
        "Problem 3 (this is difficult so don't worry if you cannot solve it now. Give it a go and then come back to it later in the semester when you have more programming experience. It will be a good check on your progress):\n",
        "\n",
        "Estimate a trigram word-based language model. This will require smoothing and you can employ both kinds. You should make use of the code you wrote for character-based trigram language models and the examples of smoothing above."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Generating with language models\n",
        "\n",
        "Generation with language models is also sometimes called auto-regressive generation. It works by selecting and then outputting words from the vocabulary based on their probability given a preceding context - either (at time point 1) a prompt from the user, or (at subsequent time points) the prompt plus the words that have been generated so far.\n",
        "\n",
        "There are however a wide range of ways in which they are chosen - different methods of what is know as \"decoding\".\n",
        "\n",
        "While real-world language models generate words based on neural language models (e.g. transformers) we can separate the underlying model from the decoding process."
      ],
      "metadata": {
        "id": "H2qojCTJsUgt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "nested_dict = lambda: defaultdict(nested_dict)\n",
        "d = nested_dict()\n",
        "\n",
        "for bg in bigrams:\n",
        "  ug = bg.split()\n",
        "  d[ug[0]][ug[1]] = np.log(bigrams[bg]/unigrams[ug[0]])\n",
        "\n",
        "lm=pd.DataFrame(d)"
      ],
      "metadata": {
        "id": "KbZrNey_tTuz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Generation by Greedy search"
      ],
      "metadata": {
        "id": "Gypxln9Hs48X"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define starting word\n",
        "w=\"he\"\n",
        "# Define stopping point - here when an end of line character is output or length reaches 15\n",
        "length = 0\n",
        "while w != \"eol\" and length <= 15:\n",
        "  length += 1\n",
        "  print(w,end=' ')\n",
        "  # get probabilities for all words following the previous word\n",
        "  s=lm[w]\n",
        "\n",
        "  # sort the probabilities and output the most likely word\n",
        "  w=s.sort_values(ascending=False).index[0]\n"
      ],
      "metadata": {
        "id": "aIvM_8hLs0y_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Generation by sampling"
      ],
      "metadata": {
        "id": "3MmaNVpbs76R"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "# Specify starting word\n",
        "w=\"he\"\n",
        "length=0\n",
        "# Define stopping point - here when an end of line character is output or length reaches 15 words\n",
        "while w != \"eol\" and length <= 15:\n",
        "  length += 1\n",
        "  print(w,end=' ')\n",
        "  # get probabilities for all words following the previous word\n",
        "  s=lm.loc[w]\n",
        "  s=s.drop(s[np.isnan(s)].index)\n",
        "  # Choose randomly from the probability distribution over next words\n",
        "  w=np.random.choice(list(s.index),1,list(np.exp(s.values)))[0]"
      ],
      "metadata": {
        "id": "AvX_wl7KtD2m"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}