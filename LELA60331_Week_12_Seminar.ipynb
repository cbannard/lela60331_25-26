{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "513845ed-b955-407f-bd4f-41703c1b1cb7",
      "metadata": {
        "id": "513845ed-b955-407f-bd4f-41703c1b1cb7"
      },
      "source": [
        "# LELA 60331  Computational Linguistics 1\n",
        "### Week 12Â¶\n",
        "\n",
        "Today we are going to use Pytorch to perform classification with sequence models.\n",
        "\n",
        "The first dataset we are going to work with consists of just over 10,000 surnames, labelled with 18 different nationalities. The first tasks will be to learn a classifier that can accurately assign a nationality to previously unseen surnames. To do this we will use RNNs."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "! wget https://raw.githubusercontent.com/cbannard/lela60342/refs/heads/main/surnames_data.csv"
      ],
      "metadata": {
        "id": "n3m4wC-35-b8"
      },
      "id": "n3m4wC-35-b8",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We read the data into a Pandas dataframe:"
      ],
      "metadata": {
        "id": "kP_QQHszscJB"
      },
      "id": "kP_QQHszscJB"
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import torch\n",
        "surnames_df=pd.read_csv(\"surnames_data.csv\")\n",
        "surnames_df"
      ],
      "metadata": {
        "id": "_IT9f0pwsOeh"
      },
      "id": "_IT9f0pwsOeh",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We then use hierachical indexing in Pandas to represent the data as sequences of separate characters"
      ],
      "metadata": {
        "id": "6y-35DGhRNWZ"
      },
      "id": "6y-35DGhRNWZ"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pCd8xn0NMZAx"
      },
      "source": [
        "### Hierachical indexing\n",
        "\n",
        "One pandas feature that you will find useful in representing language data is hierachical indexing."
      ],
      "id": "pCd8xn0NMZAx"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DuiCRW_wMRqf"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "s=pd.Series(np.random.randn(16),index=[[1,1,1,1,2,2,2,2,3,3,3,3,4,4,4,4],[\"a\",\"b\",\"c\",\"d\",\"a\",\"b\",\"c\",\"d\",\"a\",\"b\",\"c\",\"d\",\"a\",\"b\",\"c\",\"d\"]])\n",
        "s"
      ],
      "id": "DuiCRW_wMRqf"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4m_4L2YFSMnH"
      },
      "source": [
        "We can select subsets from the hierachical index as follows:"
      ],
      "id": "4m_4L2YFSMnH"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BxA05Xc3Sk1X"
      },
      "outputs": [],
      "source": [
        "s.loc[1]"
      ],
      "id": "BxA05Xc3Sk1X"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n_AzMtoVTHDJ"
      },
      "outputs": [],
      "source": [
        "s.loc[1,\"a\"]"
      ],
      "id": "n_AzMtoVTHDJ"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qDOPuw-XTAAX"
      },
      "outputs": [],
      "source": [
        "s.loc[:,\"a\"]"
      ],
      "id": "qDOPuw-XTAAX"
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can use this to represent our data here where characters belong to words"
      ],
      "metadata": {
        "id": "mu6ZxqbUzbiK"
      },
      "id": "mu6ZxqbUzbiK"
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "surnames_df=pd.read_csv(\"surnames_data.csv\")\n",
        "\n",
        "chars=[]\n",
        "index_1=[]\n",
        "index_2=[]\n",
        "for i,row in surnames_df.iterrows():\n",
        "    chars.extend(list(row.surname))\n",
        "    index_1.extend([i]*len(row.surname))\n",
        "    index_2.extend(range(len(row.surname)))\n",
        "\n",
        "surnames_chars = pd.DataFrame(chars,index=[index_1,index_2])\n",
        "surnames_chars.columns = [\"chars\"]\n",
        "surnames_chars"
      ],
      "metadata": {
        "id": "fwHsPLy56JJF"
      },
      "id": "fwHsPLy56JJF",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can then use the Pandas function get_dummies to produce one-hot codings of the characters"
      ],
      "metadata": {
        "id": "R9iLfifss0Yo"
      },
      "id": "R9iLfifss0Yo"
    },
    {
      "cell_type": "code",
      "source": [
        "surnames_oh=pd.get_dummies(surnames_chars.chars,dtype=int)\n",
        "surnames_oh"
      ],
      "metadata": {
        "id": "pxLril3Gsuje"
      },
      "id": "pxLril3Gsuje",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "And the nationalities"
      ],
      "metadata": {
        "id": "AFKV89B3tGkb"
      },
      "id": "AFKV89B3tGkb"
    },
    {
      "cell_type": "code",
      "source": [
        "nationalities_oh=pd.get_dummies(surnames_df.nationality,dtype=int)\n",
        "nationalities_oh"
      ],
      "metadata": {
        "id": "PSTAY8bxs92w"
      },
      "id": "PSTAY8bxs92w",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We will then turn these into tensors for input to PyTorch and in particular to an LSTM layer. We want a tensor with the shape [Number_of_names, Number_of_characters_in name, Size_of_alphabet].\n",
        "\n",
        "However the LSTM layer requires that all sequence be of the same length and so we pad our tensors by adding N tensors of zeros of the length of the one hot codings to the beginning of each name. So that the tensor actually has the form [Number_of_names, Number_of_characters_in_the_longest_name, Size_of_alphabet]\n",
        "\n",
        "We do this using the function ZeroPad1d which takes as an argument a tuple with the following entries: padding_left, padding_right, padding_above, padding below.\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "TXS6br9XtQne"
      },
      "id": "TXS6br9XtQne"
    },
    {
      "cell_type": "code",
      "source": [
        "from torch import nn\n",
        "t=torch.ones([5,5,5])\n",
        "print(t[0,:,:])\n",
        "m = nn.ZeroPad1d((0,0,2,0))\n",
        "print(m(t))"
      ],
      "metadata": {
        "id": "k3S5qErrFpyM"
      },
      "id": "k3S5qErrFpyM",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch import nn\n",
        "# Find the length of the longest name in the data:\n",
        "max_length=max([t[1] for t in surnames_oh.index])\n",
        "# Make an array for the name tensors\n",
        "X = [0] * (max(surnames_oh.index)[0]+1)\n",
        "# Make an array for the label tensors\n",
        "y = [0] * (max(nationalities_oh.index)+1)\n",
        "# Iterate over index of the surnames one-hot data frame. The indices are tuples.\n",
        "for ind in surnames_oh.index:\n",
        "    # Make a tensor from subset of the dataframe for this name/index\n",
        "    s=torch.from_numpy(surnames_oh.loc[ind[0]].values).to(dtype=torch.float)\n",
        "    # Pad the tensor\n",
        "    m = nn.ZeroPad1d((0,0,max_length-len(s),0))\n",
        "    # Add tensors to arrays\n",
        "    X[ind[0]] = m(s).cuda()\n",
        "    y[ind[0]] = torch.from_numpy(nationalities_oh.loc[ind[0]].values).to(dtype=torch.float).cuda()\n",
        "# Combine contents of arrays into a single tensor\n",
        "X=torch.stack(X)\n",
        "y=torch.stack(y)"
      ],
      "metadata": {
        "id": "q_O3VAIvD6Wo"
      },
      "id": "q_O3VAIvD6Wo",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X.shape"
      ],
      "metadata": {
        "id": "vclVN6WKulAH"
      },
      "id": "vclVN6WKulAH",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y.shape"
      ],
      "metadata": {
        "id": "be8pT09funpG"
      },
      "id": "be8pT09funpG",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### RNN layers in PyTorch\n",
        "\n",
        "RNN layers can be specified as follows. We need to specify the size of the input (e.g. the length our one-hot vectors), the size of the hidden layer to use, the number of layers to include. And because of the way that our data is configured - (batch, seq, feature) rather than (seq, batch, feature) - we use the batch_first flag.\n"
      ],
      "metadata": {
        "id": "abr-O-JLuzMz"
      },
      "id": "abr-O-JLuzMz"
    },
    {
      "cell_type": "code",
      "source": [
        "input=torch.randn((1,10,10))\n",
        "rnn = nn.RNN(input_size=10, hidden_size=5, num_layers=1, batch_first=True)\n",
        "hidden, output =rnn(input)\n",
        "print(output)"
      ],
      "metadata": {
        "id": "jPbVOE8au5sr"
      },
      "id": "jPbVOE8au5sr",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The output is the final hidden layers from each step of the sequence. The second element output is a tuple containing the hidden states from all layers and timepoints. Here we are interested in the hidden layer values as it is the hidden layer from final step for each sequence that we will pass on to a linear layer to perform classification. We could take this from either the output or the hidden objects. In our code we take this from the output."
      ],
      "metadata": {
        "id": "8NuBmdaExxvW"
      },
      "id": "8NuBmdaExxvW"
    },
    {
      "cell_type": "code",
      "source": [
        "hidden"
      ],
      "metadata": {
        "id": "Zc9FU6-XvMEu"
      },
      "id": "Zc9FU6-XvMEu",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can use torch.nn.Module to define our whole model"
      ],
      "metadata": {
        "id": "Hca5QMfW1059"
      },
      "id": "Hca5QMfW1059"
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.utils.data as data\n",
        "n_classes = 18\n",
        "\n",
        "class SeqModel(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.rnn = nn.RNN(input_size=84, hidden_size=42, num_layers=1, batch_first=True)\n",
        "        self.linear = nn.Linear(42, n_classes)\n",
        "    def forward(self, x):\n",
        "        x, _ = self.rnn(x)\n",
        "        # take only the last output\n",
        "        x = x[:, -1, :]\n",
        "        x = self.linear(x)\n",
        "        return x"
      ],
      "metadata": {
        "id": "-tVhkDSPyT-g"
      },
      "id": "-tVhkDSPyT-g",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we have the model we can split the data then train and then test. We will use CrossEntropyLoss because our output is an 18-class softmax. We will use batch training."
      ],
      "metadata": {
        "id": "85ddcHHby_5z"
      },
      "id": "85ddcHHby_5z"
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=30)"
      ],
      "metadata": {
        "id": "fm2Ccx7Gha94"
      },
      "id": "fm2Ccx7Gha94",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.utils import gen_batches\n",
        "import matplotlib.pyplot as plt\n",
        "n_epochs = 150\n",
        "batch_size = 128\n",
        "model = SeqModel()\n",
        "model.to(\"cuda\")\n",
        "ce_loss=[]\n",
        "optimizer = optim.Adam(model.parameters(),lr=0.005)\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "\n",
        "for i in range(n_epochs):\n",
        "    cumul_loss = 0.0\n",
        "    batches = gen_batches(X_train.shape[0],batch_size)\n",
        "    cumul_loss=0.0\n",
        "    for k in batches:\n",
        "          inputs=X_train[k]\n",
        "          outputs=y_train[k]\n",
        "          y_pred = model(inputs)\n",
        "          loss = loss_fn(y_pred, outputs)\n",
        "          optimizer.zero_grad()\n",
        "          loss.backward()\n",
        "          optimizer.step()\n",
        "          cumul_loss += loss.item()\n",
        "    ce_loss.append(cumul_loss)\n",
        "\n",
        "plt.plot(range(1,n_epochs),ce_loss[1:])\n",
        "plt.xlabel(\"number of epochs\")\n",
        "plt.ylabel(\"loss\")\n",
        "\n"
      ],
      "metadata": {
        "id": "TJEiFBuI5-od"
      },
      "id": "TJEiFBuI5-od",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Our precision and recall are as follows:"
      ],
      "metadata": {
        "id": "jSqBSq9JzVMH"
      },
      "id": "jSqBSq9JzVMH"
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.metrics import precision_recall_fscore_support\n",
        "y_test_pred=[np.argmax(x.cpu().detach().numpy()) for x in model(X_test)]\n",
        "y_test_int=[np.argmax(x.cpu().detach().numpy()) for x in y_test]\n",
        "precision_recall_fscore_support(y_test_int, y_test_pred, average='macro')\n"
      ],
      "metadata": {
        "id": "1qsKrjfflUdt"
      },
      "id": "1qsKrjfflUdt",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can try the model out on individual names as follows:"
      ],
      "metadata": {
        "id": "yZ4DTG3WzX8u"
      },
      "id": "yZ4DTG3WzX8u"
    },
    {
      "cell_type": "code",
      "source": [
        "name=\"bannard\"\n",
        "torch.manual_seed(42)\n",
        "charset=list(surnames_oh.columns.values)\n",
        "nationalities=list(nationalities_oh.columns.values)\n",
        "oh = torch.zeros(16,len(charset))\n",
        "for i,c in enumerate(name):\n",
        "    oh[16-len(name)+i,charset.index(c)] = 1.0\n",
        "oh=oh.to(\"cuda\")\n",
        "print(oh.shape)\n",
        "pred=model(torch.unsqueeze(oh,0))\n",
        "nationalities[np.argmax(pred.cpu().detach().numpy())]"
      ],
      "metadata": {
        "id": "36tN7cPXmgMS"
      },
      "id": "36tN7cPXmgMS",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can examine the model and its weights as follows:"
      ],
      "metadata": {
        "id": "YjGiUFrz6Qk4"
      },
      "id": "YjGiUFrz6Qk4"
    },
    {
      "cell_type": "code",
      "source": [
        "model"
      ],
      "metadata": {
        "id": "HdyUb_I46PzR"
      },
      "id": "HdyUb_I46PzR",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.state_dict()"
      ],
      "metadata": {
        "id": "mWwR6AGe6usD"
      },
      "id": "mWwR6AGe6usD",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can try to improve performance by using a gated RNN, specifically an LSTM (see week 12 lecture)"
      ],
      "metadata": {
        "id": "getBAK2EHcVD"
      },
      "id": "getBAK2EHcVD"
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.utils.data as data\n",
        "n_classes = 18\n",
        "\n",
        "class SeqModel(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.lstm = nn.LSTM(input_size=84, hidden_size=42, num_layers=1, batch_first=True)\n",
        "        self.linear = nn.Linear(42, n_classes)\n",
        "    def forward(self, x):\n",
        "        x, _ = self.lstm(x)\n",
        "        # take only the last output\n",
        "        x = x[:, -1, :]\n",
        "        x = self.linear(x)\n",
        "        return x"
      ],
      "metadata": {
        "id": "uuerFEle_iAm"
      },
      "id": "uuerFEle_iAm",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.utils import gen_batches\n",
        "import matplotlib.pyplot as plt\n",
        "n_epochs = 150\n",
        "batch_size = 128\n",
        "model = SeqModel()\n",
        "model.to(\"cuda\")\n",
        "ce_loss=[]\n",
        "optimizer = optim.Adam(model.parameters(),lr=0.005)\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "\n",
        "\n",
        "for i in range(n_epochs):\n",
        "    cumul_loss = 0.0\n",
        "    batches = gen_batches(X_train.shape[0],batch_size)\n",
        "    cumul_loss=0.0\n",
        "    for k in batches:\n",
        "          inputs=X_train[k]\n",
        "          outputs=y_train[k]\n",
        "          y_pred = model(inputs)\n",
        "          loss = loss_fn(y_pred, outputs)\n",
        "          optimizer.zero_grad()\n",
        "          loss.backward()\n",
        "          optimizer.step()\n",
        "          cumul_loss += loss.item()\n",
        "    ce_loss.append(cumul_loss)\n",
        "\n",
        "plt.plot(range(1,n_epochs),ce_loss[1:])\n",
        "plt.xlabel(\"number of epochs\")\n",
        "plt.ylabel(\"loss\")\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "YcFG-xtr_pk5"
      },
      "id": "YcFG-xtr_pk5",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.metrics import precision_recall_fscore_support\n",
        "y_test_pred=[np.argmax(x.cpu().detach().numpy()) for x in model(X_test)]\n",
        "y_test_int=[np.argmax(x.cpu().detach().numpy()) for x in y_test]\n",
        "precision_recall_fscore_support(y_test_int, y_test_pred, average='macro')"
      ],
      "metadata": {
        "id": "QwW5znEUICjS"
      },
      "id": "QwW5znEUICjS",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "name=\"bannard\"\n",
        "torch.manual_seed(42)\n",
        "charset=list(surnames_oh.columns.values)\n",
        "nationalities=list(nationalities_oh.columns.values)\n",
        "oh = torch.zeros(16,len(charset))\n",
        "for i,c in enumerate(name):\n",
        "    oh[16-len(name)+i,charset.index(c)] = 1.0\n",
        "oh=oh.to(\"cuda\")\n",
        "print(oh.shape)\n",
        "pred=model(torch.unsqueeze(oh,0))\n",
        "nationalities[np.argmax(pred.cpu().detach().numpy())]"
      ],
      "metadata": {
        "id": "7ZmoTDcMIHZh"
      },
      "id": "7ZmoTDcMIHZh",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Review classification with RNN\n",
        "\n",
        "\n",
        "Next we will apply the same process to the Yelp review sentiment data that we have been working with all semester.\n",
        "\n",
        "In order to speed things up I have prepared the Tensors that you need from the raw data. I have also only used a random sample of 1000 reviews to make training time manageable in class."
      ],
      "metadata": {
        "id": "yufZwNjlFWSr"
      },
      "id": "yufZwNjlFWSr"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x5tc3ZAv1_0a"
      },
      "outputs": [],
      "source": [
        "!gdown 19cYQ_B3diu6RqlpYT5n9qHvS08_cScp7\n",
        "!gdown 1DHj5zFiWX3hF3o8RxMx2Hn4sOW-VsHTs\n",
        "\n",
        "!gunzip reviews_for_rnn.pt.gz"
      ],
      "id": "x5tc3ZAv1_0a"
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "reviews_emb=torch.load(\"reviews_for_rnn.pt\")\n",
        "labels=torch.load(\"review_labels_for_rnn.pt\")"
      ],
      "metadata": {
        "id": "P_YlcLRr2daO"
      },
      "execution_count": null,
      "outputs": [],
      "id": "P_YlcLRr2daO"
    },
    {
      "cell_type": "markdown",
      "source": [
        "The tokens in the reviews are represented using 300 element static embedding vectors. The longest review is 887 tokens long so we pad all the sentence vectors to this length. There are 1000 reviews. So the input data is a 1000x887x300 3D tensor."
      ],
      "metadata": {
        "id": "JHP1Zt2S7i0a"
      },
      "id": "JHP1Zt2S7i0a"
    },
    {
      "cell_type": "code",
      "source": [
        "reviews_emb.shape"
      ],
      "metadata": {
        "id": "iK39NAUQ69xR"
      },
      "id": "iK39NAUQ69xR",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We split this into an 800 review training set and a 200 review test set as follows"
      ],
      "metadata": {
        "id": "xoNPkONV9K7G"
      },
      "id": "xoNPkONV9K7G"
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "X_train, X_test, y_train, y_test = train_test_split(reviews_emb, labels, test_size=0.2, random_state=30)\n",
        "X_train = X_train.to(\"cuda\")\n",
        "X_test = X_test.to(\"cuda\")\n",
        "y_train = y_train.to(\"cuda\")\n",
        "y_test = y_test.to(\"cuda\")\n"
      ],
      "metadata": {
        "id": "Ri0SS_9LIzRf"
      },
      "id": "Ri0SS_9LIzRf",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Problem 1: Build an LSTM-based classifier for this review data. Note: this is a binary classifier so you will need to change the loss function and pass the output of your model x through torch.sigmoid()"
      ],
      "metadata": {
        "id": "OQhoCLRg9Ux9"
      },
      "id": "OQhoCLRg9Ux9"
    },
    {
      "cell_type": "markdown",
      "source": [
        "Once you have build it you can evaluate it as follows"
      ],
      "metadata": {
        "id": "kRSpYDfC9kgo"
      },
      "id": "kRSpYDfC9kgo"
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.metrics import precision_score, recall_score\n",
        "labels_pred=[int(x.cpu().detach().numpy() > 0.5) for x in model(X_test)]\n",
        "#precision_recall_fscore_support(y_test.cpu().detach().numpy(), np.array(labels_pred))\n",
        "print(precision_score(y_test.cpu().detach().numpy(), np.array(labels_pred)))\n",
        "print(recall_score(y_test.cpu().detach().numpy(), np.array(labels_pred)))\n"
      ],
      "metadata": {
        "id": "cuB_bwvU9u8l"
      },
      "id": "cuB_bwvU9u8l",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Intent classification with RNNs\n",
        "\n",
        "Now we are going to work with some sentences - utterances input to a dialogue system assigned with the speaker intent.\n",
        "\n",
        "'PlayMusic', e.g. \"play easy listening\" \\\n",
        "'AddToPlaylist' e.g. \"please add this song to road trip\" \\\n",
        "'RateBook' e.g. \"give this novel 5 stars\" \\\n",
        "'SearchScreeningEvent' e.g. \"give me a list of local movie times\" \\\n",
        "'BookRestaurant' e.g. \"i'd like a table for four at 7pm at Asti\" \\\n",
        "'GetWeather' e.g. \"what's it like outside\" \\\n",
        "'SearchCreativeWork' \"show me the new James Bond trailer\"\n",
        "\n"
      ],
      "metadata": {
        "id": "GQoPCi6UJxQL"
      },
      "id": "GQoPCi6UJxQL"
    },
    {
      "cell_type": "markdown",
      "source": [
        "*Problem* 2: Build and train an RNN-based classifier using a training subset of the data that can correctly classify a test subset of the data."
      ],
      "metadata": {
        "id": "fmxSNjClKYod"
      },
      "id": "fmxSNjClKYod"
    },
    {
      "cell_type": "markdown",
      "source": [
        "I have prebuilt the tensors containing word embeddings for you:"
      ],
      "metadata": {
        "id": "or-WrAH8KtmW"
      },
      "id": "or-WrAH8KtmW"
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://raw.githubusercontent.com/cbannard/lela60342/refs/heads/main/utts_emb.pt.gz\n",
        "!gunzip utts_emb.pt.gz\n",
        "!wget https://raw.githubusercontent.com/cbannard/lela60342/refs/heads/main/intents_emb.pt\n",
        "X_utts=torch.load(\"utts_emb.pt\")\n",
        "y_intents=torch.load(\"intents_emb.pt\")"
      ],
      "metadata": {
        "id": "BEeT0FQIHStr"
      },
      "id": "BEeT0FQIHStr",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_utts.shape"
      ],
      "metadata": {
        "id": "T9dzUOK3H_Lf"
      },
      "id": "T9dzUOK3H_Lf",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_intents.shape"
      ],
      "metadata": {
        "id": "hvOm7pFUIfUo"
      },
      "id": "hvOm7pFUIfUo",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_utts, y_intents, test_size=0.2, random_state=30)\n",
        "X_train=X_train.to(\"cuda\")\n",
        "X_test=X_test.to(\"cuda\")\n",
        "y_train=y_train.to(\"cuda\")\n",
        "y_test=y_test.to(\"cuda\")\n"
      ],
      "metadata": {
        "id": "7lef6r6KIiWi"
      },
      "id": "7lef6r6KIiWi",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Problem 3: Starting from the Pandas data_frame intent_classification (imported as below), compile the data into the format needed by your model. Note that the utterances are of different lengths so you will need to do some padding. The data frame is hierachically indexed for utterance and word, so that the format is almost identical to the name data. Once you have compiled the data use it to train your model above."
      ],
      "metadata": {
        "id": "Ap-yO1yLLOA5"
      },
      "id": "Ap-yO1yLLOA5"
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://raw.githubusercontent.com/cbannard/lela60342/refs/heads/main/intent_classification.pickle"
      ],
      "metadata": {
        "id": "DMA3c9qPNxVc"
      },
      "id": "DMA3c9qPNxVc",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "intent_classification = pd.read_pickle(\"intent_classification.pickle\")\n"
      ],
      "metadata": {
        "id": "l0hNAdUSN8au"
      },
      "id": "l0hNAdUSN8au",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "intent_classification"
      ],
      "metadata": {
        "id": "7-oT4MeQOFGM"
      },
      "id": "7-oT4MeQOFGM",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.4"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}