{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Week 11 Seminar Notebook: Training a Softmax Classifier\n",
        "\n",
        "Today we are going to look again at model training and evaluation, starting by looking briefly at training with the simulated dataset we worked with last week and then applying what we learn to a new real language dataset.\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "61td5eSnl5aI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "First we need to create the simulated dataset"
      ],
      "metadata": {
        "id": "gzkxuED4Au40"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rfJwFw6wBXTp"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "## Create simulated data\n",
        "np.random.seed(10)\n",
        "w1_center = (1, 3)\n",
        "w2_center = (3, 1)\n",
        "w3_center = (1, 1)\n",
        "w4_center = (3, 3)\n",
        "\n",
        "x=np.concatenate((np.random.normal(loc=w1_center,size=(20,2)),np.random.normal(loc=w2_center,size=(20,2)),np.random.normal(loc=w3_center,size=(10,2)),np.random.normal(loc=w4_center,size=(10,2))))\n",
        "\n",
        "labs=np.repeat([0,1,2],[20,20,20],axis=0)\n",
        "y=np.repeat(np.diag((1,1,1)),[20,20,20],axis=0)\n",
        "x=x.T\n",
        "x=np.array([x[0],x[1],[1] * len(x[0])])\n",
        "\n",
        "plt.scatter(x[0][labs==0], x[1][labs==0], marker='*', s=100)\n",
        "plt.scatter(x[0][labs==1], x[1][labs==1], marker='o', s=100)\n",
        "plt.scatter(x[0][labs==2], x[1][labs==2], marker='x', s=100)\n",
        "plt.xlabel(\"log count of negative words\")\n",
        "plt.ylabel(\"log count of positive words\")\n",
        "plt.xlim((0,5))\n",
        "plt.ylim((0,5))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The input matrix x has the following shape:"
      ],
      "metadata": {
        "id": "q-xXmt76-b_I"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "x.shape"
      ],
      "metadata": {
        "id": "pqANm4tM-ea8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This is a matrix with 3 rows (the input features including the dummy row of ones for the bias node) with 60 columns - one for each data points. So each column is paired with a target outcome value in y"
      ],
      "metadata": {
        "id": "Ywz4ZZmz-m8-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "y.shape"
      ],
      "metadata": {
        "id": "b1Kk8vPb-4ij"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The following code trains a multiclass classifier to convergence.  Note that before I run it I am going to transpose the matrix so that it has 60 rows and 3 columns as this will be easier to deal with."
      ],
      "metadata": {
        "id": "PUDXSNYb-mZC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "x=x.T\n",
        "x.shape"
      ],
      "metadata": {
        "id": "h3VU4SLYoj9s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "np.random.seed(10)\n",
        "n_iters = 1000\n",
        "num_features=3\n",
        "num_classes=3\n",
        "num_samples = len(y)\n",
        "weights = np.random.rand(num_classes,num_features)\n",
        "lr=0.2\n",
        "logistic_loss=[]\n",
        "z=np.zeros((num_samples,num_classes))\n",
        "q=np.zeros((num_samples,num_classes))\n",
        "\n",
        "for i in range(n_iters):\n",
        "\n",
        "    z= x.dot(weights)\n",
        "    z_sum=np.exp(z).sum(axis=1)\n",
        "    q=np.array([list(np.exp(z_i)/z_sum[i]) for i, z_i in enumerate(z)])\n",
        "    #print(q)\n",
        "    loss=np.mean(-np.log2((np.sum((y*q),axis=1))))\n",
        "\n",
        "    logistic_loss.append(loss)\n",
        "\n",
        "    dw=x.T.dot((q-y))/num_samples\n",
        "    weights=(weights - (dw*lr))\n",
        "\n",
        "plt.plot(range(1,n_iters),logistic_loss[1:])\n",
        "plt.xlabel(\"number of epochs\")\n",
        "plt.ylabel(\"loss\")"
      ],
      "metadata": {
        "id": "-j2K56MG_5oa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "In the following block I am going to make a slight adjustment to the code, so that it uses batch training - instead of updating my weights after each full pass through the data I am going to break the data up into equally sized batches and update the weights after each pass through each batch."
      ],
      "metadata": {
        "id": "bHHhXWBe_gJN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "np.random.seed(10)\n",
        "n_iters = 1000\n",
        "num_features=3\n",
        "num_classes=3\n",
        "num_samples = len(y)\n",
        "weights = np.random.rand(num_classes,num_features)\n",
        "lr=0.2\n",
        "logistic_loss=[]\n",
        "z=np.zeros((num_samples,num_classes))\n",
        "q=np.zeros((num_samples,num_classes))\n",
        "\n",
        "# Create batches\n",
        "import random\n",
        "batch_size=12\n",
        "batch_assignments=[0,1,2,3,4]*batch_size\n",
        "random.shuffle(batch_assignments)\n",
        "\n",
        "for i in range(n_iters):\n",
        "   cumulative_loss = 0.0\n",
        "   # For each epoch I run through the batches, updating weights each time\n",
        "   for j in range(int(len(y)/batch_size)):\n",
        "       input = x[[ind for ind, v in enumerate(batch_assignments) if v == j]]\n",
        "       targets = y[[ind for ind, v in enumerate(batch_assignments) if v == j]]\n",
        "       z= input.dot(weights)\n",
        "       z_sum=np.exp(z).sum(axis=1)\n",
        "       q=np.array([list(np.exp(z_i)/z_sum[i]) for i, z_i in enumerate(z)])\n",
        "       #print(q)\n",
        "       loss=np.mean(-np.log2((np.sum((targets*q),axis=1))))\n",
        "       cumulative_loss+=loss\n",
        "       dw=input.T.dot((q-targets))/num_samples\n",
        "       weights=(weights - (dw*lr))\n",
        "   logistic_loss.append(cumulative_loss)\n",
        "plt.plot(range(1,n_iters),logistic_loss[1:])\n",
        "plt.xlabel(\"number of epochs\")\n",
        "plt.ylabel(\"loss\")"
      ],
      "metadata": {
        "id": "56zHb9hcOqmy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Building an intent classifier\n",
        "\n",
        "Below you can find data for the task of intent classification. This is a step in task-based dialogue systems - given a particular piece of input from the speaker, the system tries to determine what goal the speaker is trying to achieve, in order that it can then produce an appropriate response.\n",
        "\n",
        "The data set pairs 2100 reviews with one of seven different intents:\n",
        "\n",
        "'PlayMusic', e.g. \"play easy listening\"  \\\n",
        "'AddToPlaylist' e.g. \"please add this song to road trip\" \\\n",
        "'RateBook' e.g. \"give this novel 5 stars\" \\\n",
        "'SearchScreeningEvent' e.g. \"give me a list of local movie times\" \\\n",
        "'BookRestaurant' e.g. \"i'd like a table for four at 7pm at Asti\" \\\n",
        "'GetWeather' e.g. \"what's it like outside\" \\\n",
        "'SearchCreativeWork' \"show me the new James Bond trailer\"\n",
        "\n",
        "To import the data a prepare it for training please run the following cell.\n"
      ],
      "metadata": {
        "id": "nsJveI-yYg4R"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "! wget https://raw.githubusercontent.com/cbannard/lela60331_24-25/refs/heads/main/intent_classification.csv\n",
        "# Or in Jupyter\n",
        "#from urllib.request import urlretrieve\n",
        "#url = \"https://raw.githubusercontent.com/cbannard/lela60331_24-25/refs/heads/main/intent_classification.csv\"\n",
        "#filename=\"intent_classification.csv\"\n",
        "#urlretrieve(url, filename)"
      ],
      "metadata": {
        "id": "yqznaPDmMgoS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "! wget https://raw.githubusercontent.com/cbannard/lela60331_24-25/refs/heads/main/intent_classification.csv\n",
        "# Or in Jupyter\n",
        "#from urllib.request import urlretrieve\n",
        "#url = \"https://raw.githubusercontent.com/cbannard/lela60331_24-25/refs/heads/main/intent_classification.csv\"\n",
        "#filename=\"intent_classification.csv\"\n",
        "#urlretrieve(url, filename)\n",
        "\n",
        "# Import the data into utterances and labels lists\n",
        "\n",
        "utterances=[]\n",
        "labels=[]\n",
        "\n",
        "with open(\"intent_classification.csv\") as f:\n",
        "   # iterate over the lines in the file\n",
        "   for line in f.readlines()[1:]:\n",
        "        # split the current line into a list of two element - the review and the label\n",
        "        fields = line.rstrip().split(',')\n",
        "        # put the current review in the reviews list\n",
        "        utterances.append(fields[0])\n",
        "        # put the current sentiment rating in the labels list\n",
        "        labels.append(fields[1])\n",
        "## Select vocabulary for inclusion in one-hot representations\n",
        "from collections import Counter\n",
        "import re\n",
        "import numpy as np\n",
        "# Tokenise the text, turning a list of strings into a list of lists of tokens. We use very naive space-based tokenisation.\n",
        "tokenized_sents = [re.findall(\"[^ ]+\",txt) for txt in utterances]\n",
        "# Collapse all tokens into a single list\n",
        "tokens=[]\n",
        "for s in tokenized_sents:\n",
        "      tokens.extend(s)\n",
        "# Count the tokens in the tokens list. The returns a list of tuples of each token and count\n",
        "counts=Counter(tokens)\n",
        "# Sort the tuples. The reverse argument instructs to put most frequent first rather than last (which is the default)\n",
        "so=sorted(counts.items(), key=lambda item: item[1], reverse=True)\n",
        "# Extract the list of tokens, by transposing the list of lists so that there is a list of tokens a list of counts and then just selecting the former\n",
        "so=list(zip(*so))[0]\n",
        "# Select the firs 5000 words in the list\n",
        "type_list=so[0:2500]\n",
        "\n",
        "# Create a 2100 x 2500 matrix of zeros\n",
        "M = np.zeros((len(utterances), len(type_list)))\n",
        "#iterate over the reviews\n",
        "for i, utt in enumerate(utterances):\n",
        "    # Tokenise the current review:\n",
        "    tokens = re.findall(\"[^ ]+\",utt)\n",
        "    # iterate over the words in our type list (the set of 5000 words):\n",
        "    for j,t in enumerate(type_list):\n",
        "        # if the current word j occurs in the current review i then set the matrix element at i,j to be one. Otherwise leave as zero.\n",
        "        if t in tokens:\n",
        "              M[i,j] = 1"
      ],
      "metadata": {
        "id": "6eSkDiJhM7Y9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The data is now a matrix in which each row is a training instance and each column represents the presence or absence of one of the one-hot-coded words."
      ],
      "metadata": {
        "id": "4qGm7SwoHIa-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "M.shape"
      ],
      "metadata": {
        "id": "RvbGFBbjHM6v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "I want to split this into training, development and test data. I first of all select a random sample of 80% of the ints between 0 and the sample size. I then obtain a list of the remaining ints and splits this in half so that 10% of the data is a development set and the last 10% is the test data. These can be used to split the data into training set of 1680, a development set of 210 and a test set of 210."
      ],
      "metadata": {
        "id": "BhbXEjzIG9w-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_ints=np.random.choice(len(utterances),int(len(utterances)*0.8),replace=False)\n",
        "remaining_ints=list(set(range(0,len(utterances))) - set(train_ints))\n",
        "test_ints=np.random.choice(len(remaining_ints),int(len(remaining_ints)*0.5),replace=False)\n",
        "dev_ints=list(set(range(0,len(remaining_ints))) - set(test_ints))"
      ],
      "metadata": {
        "id": "RH1JA9zGN0Ov"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "I then use these ints to select 3 sets of rows of data, resulting in training, development and test data inputs. I use the same ints to select the targets for each set of training items from the labels array. And finally I one-hot encode these targets."
      ],
      "metadata": {
        "id": "Z2iwmJeyH6I7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "### Generate one-hot encoded input matrices for training, development and test\n",
        "M_train = np.array(M[train_ints,]).T\n",
        "M_test = np.array(M[test_ints,]).T\n",
        "M_dev = np.array(M[dev_ints,]).T\n",
        "\n",
        "unique_labels=list(set(labels))\n",
        "unique_one_hot=np.diag(np.ones(len(unique_labels)))\n",
        "\n",
        "labels_train = [labels[i] for i in train_ints]\n",
        "labels_test = [labels[i] for i in test_ints]\n",
        "labels_dev = [labels[i] for i in dev_ints]\n",
        "\n",
        "### Generate one-hot encoded target output matrices for training, development and test\n",
        "y_train=np.array([list(unique_one_hot[k]) for k in [unique_labels.index(x) for x in labels_train]]).T\n",
        "y_test=np.array([list(unique_one_hot[k]) for k in [unique_labels.index(x) for x in labels_test]]).T\n",
        "y_dev=np.array([list(unique_one_hot[k]) for k in [unique_labels.index(x) for x in labels_dev]]).T\n",
        "\n"
      ],
      "metadata": {
        "id": "i9Za2xE8ODu9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Problem 1. Rewrite the code from the simulated data (copied below) so that it trains a softmax classifier for the intent classification task on the training set."
      ],
      "metadata": {
        "id": "OlfEjSUlaHs0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "np.random.seed(10)\n",
        "n_iters = 1000\n",
        "num_features=3\n",
        "num_classes=3\n",
        "num_samples = len(y)\n",
        "weights = np.random.rand(num_classes,num_features)\n",
        "lr=0.2\n",
        "logistic_loss=[]\n",
        "z=np.zeros((num_samples,num_classes))\n",
        "q=np.zeros((num_samples,num_classes))\n",
        "\n",
        "for i in range(n_iters):\n",
        "\n",
        "    z= x.dot(weights)\n",
        "    z_sum=np.exp(z).sum(axis=1)\n",
        "    q=np.array([list(np.exp(z_i)/z_sum[i]) for i, z_i in enumerate(z)])\n",
        "    #print(q)\n",
        "    loss=np.mean(-np.log2((np.sum((y*q),axis=1))))\n",
        "\n",
        "    logistic_loss.append(loss)\n",
        "\n",
        "    dw=x.T.dot((q-y))/num_samples\n",
        "    weights=(weights - (dw*lr))\n",
        "\n",
        "plt.plot(range(1,n_iters),logistic_loss[1:])\n",
        "plt.xlabel(\"number of epochs\")\n",
        "plt.ylabel(\"loss\")"
      ],
      "metadata": {
        "id": "G62sC-zfI6rS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Problem 2. Write code to calculate the multiclass precision and recall using macroaveraging (see week 7 lectures slides pt 2 for a definition). I've made a start for you."
      ],
      "metadata": {
        "id": "Qa6-F7a5achg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "z = weights.dot(M_test)\n",
        "z_sum=np.exp(z).sum(axis=1)\n",
        "q=np.array([list(np.exp(z_i)/z_sum[i]) for i, z_i in enumerate(z)])\n",
        "y_test_pred=np.argmax(q,axis=0)\n",
        "y_test_true=np.argmax(y_test,axis=0)\n",
        "\n",
        "TP=[]\n",
        "for j in range(7):\n",
        " TP.append(np.sum(np.array([int(s == j and y_test_true[i] == j) for i,s in enumerate(y_test_pred)])))\n"
      ],
      "metadata": {
        "id": "2vH0NbHjJiBM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Problem 3: Rewrite the code above so that it calculates precision and recall on the training data and the development data after each epoch and print these out."
      ],
      "metadata": {
        "id": "ROWIKd1nJp_S"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Problem 4: Rewrite the intent classification code so that it uses batch training."
      ],
      "metadata": {
        "id": "okxXAZ9jJ9rT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "One you have completed these problems you can move on to make a start on [this worksheet](https://githubtocolab.com/cbannard/lela60331_25-26/blob/main/LELA60331_Week_11_Worksheet_2.ipynb), which you should make sure to complete before our week 12 seminar:\n",
        "\n"
      ],
      "metadata": {
        "id": "t9TqrHnstoP8"
      }
    }
  ]
}